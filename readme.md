

1.  åœ¨dockerä¸­å®‰è£…Splash
`sudo docker pull scrapinghub/splash` 
1.  åœ¨æœ¬æœºçš„8050å’Œ8051ç«¯å£å¼€å¯SplashæœåŠ¡ï¼š
`sudo docker run -p 8050:8050 -p 8051:8051 scrapinghub/splash` 
1.  æŸ¥çœ‹å®¹å™¨æ˜¯å¦å¯åŠ¨ï¼š
`docker ps -a` 



SplashåŠŸèƒ½ä¸°å¯Œï¼ŒåŒ…å«å¤šä¸ªæœåŠ¡ç«¯ç‚¹ï¼Œè¿™é‡Œåªä»‹ç»å…¶ä¸­ä¸¤ä¸ªæœ€å¸¸ç”¨çš„ç«¯ç‚¹ï¼š


- render.htmlæä¾›JavaScripté¡µé¢æ¸²æŸ“æœåŠ¡ã€‚
- executeæ‰§è¡Œç”¨æˆ·è‡ªå®šä¹‰çš„æ¸²æŸ“è„šæœ¬ï¼ˆluaï¼‰ï¼Œåˆ©ç”¨è¯¥ç«¯ç‚¹å¯åœ¨é¡µé¢ä¸­æ‰§è¡ŒJavaScriptä»£ç ã€‚



[Splashæ–‡æ¡£åœ°å€](http://splash.readthedocs.io/en/latest/api.html)ã€‚


## splashè„šæœ¬


SplashæœåŠ¡å¼€å¯æˆåŠŸåï¼Œæµè§ˆå™¨æ‰“å¼€[http://localhost:8050/](http://localhost:8050/)ï¼Œå¦‚å›¾ï¼š
![](https://img-blog.csdnimg.cn/20200114210258848.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzE5NDczNTI5,size_16,color_FFFFFF,t_70#id=Z7XQm&originHeight=884&originWidth=1353&originalType=binary&ratio=1&status=done&style=none)


å¯ä»¥ç›´æ¥åœ¨è¿™ä¸ªé¡µé¢lua_scriptæ˜¯å¦å¯ç”¨,ä¾‹å¦‚ï¼š


demoï¼šæ¸²æŸ“çš„urlé€šè¿‡å‚æ•°ä¼ è¿›å»


```lua
function main(splash, args)
  assert(splash:go(args.url))
  assert(splash:wait(0.5))
  return {
    html = splash:html(),
    png = splash:png(),
    har = splash:har(),
  }
end
```


runjså®šä¹‰ä¸€ä¸ªå‡½æ•°,å¹¶ä¸”ä½¿ç”¨evaljsè¿è¡Œè¿™ä¸ªå‡½æ•°


```lua
function main(splash, args)
    splash:go(args.url)
    splash:runjs("foo = function() { return 'bar' }")
    local result = splash:evaljs("foo()")
    return result
end
```


åœ¨æµè§ˆå™¨æ¨¡æ‹Ÿæ‰“å¼€æ‹‰é’©ç½‘ç«™,å–æœç´¢æ¡†çš„å€¼èµ‹å€¼ç»™result1,æ¨¡æ‹Ÿæœç´¢æ¡†è¾“å…¥å…³é”®ç³»â€pythonâ€œ,å†å–æœç´¢æ¡†çš„å€¼èµ‹å€¼result2ï¼Œæ¨¡æ‹Ÿç‚¹å‡»æœç´¢æŒ‰é’®ï¼Œè·³è½¬é¡µé¢ã€‚


```lua
function main(splash, args)
    splash:go("http://www.lagou.com")
    splash:wait(2)
    splash:runjs("document.querySelector('a.tab.focus').click()")
    splash:wait(2)
    splash:runjs("getValue = function() { return document.querySelector('#search_input').value }")
    local result1 = splash:evaljs("getValue()")
    splash:runjs("keyboardInput = function (dom, st) {let evt = document.createEvent('HTMLEvents');evt.initEvent('input', true, true); dom.value = st; dom.dispatchEvent(evt);}")
    splash:evaljs("keyboardInput(document.querySelector('#search_input'), 'python')")
    local result2 = splash:evaljs("getValue()")
    splash:evaljs("document.querySelector('#search_button').click()")
    splash:wait(5)
    return {
        result1 = result1,
        result2 = result2,
        png = splash:png(),
        html = splash:html(),
    }
end
```


æ‹‰å‹¾æ¨¡æ‹Ÿç”¨æˆ·ç™»é™†ï¼Œæ‰“å¼€ç”¨æˆ·ç™»é™†çš„å¼¹çª—ï¼Œåˆ†åˆ«åœ¨ç”¨æˆ·åå’Œå¯†ç çš„è¾“å…¥æ¡†è¾“å…¥çœŸå®çš„ç”¨æˆ·åå¯†ç ã€‚å†æ¨¡æ‹Ÿç‚¹å‡»ç™»é™†æŒ‰é’®
args.url = "[https://www.lagou.com/jobs/list_python?city=6&gm=&jd=&px=default](https://www.lagou.com/jobs/list_python?city=6&gm=&jd=&px=default)"


```lua
function main(splash, args)
    splash:go(args.url)
    splash:wait(3)
  	splash:evaljs("document.querySelector('.login-pop').style.display = 'block'")
    splash:runjs("getValue = function() { return document.querySelector('input.input.login_enter_password').value }")
    splash:runjs("keyboardInput = function (dom, st) {let evt = document.createEvent('HTMLEvents');evt.initEvent('input', true, true); dom.value = st; dom.dispatchEvent(evt);}")
    splash:evaljs("keyboardInput(document.querySelector('input.input.login_enter_password'), '136****4534')")
  	splash:evaljs("keyboardInput(document.querySelector('input[type=password].input.login_enter_password'), '12345678')")
    splash:evaljs("document.querySelector('div.login-btn.login-password').click()")
    splash:wait(2)
    return {
    	text = text,
        result1 = result1,
        png = splash:png(),
    }
end
```


## åœ¨shellä¸­ä½¿ç”¨splashè„šæœ¬


å»ºè®®åœ¨shellè„šæœ¬ä¸­å¤åˆ¶ç²˜è´´ç¤ºä¾‹ä»£ç ï¼Œå› ä¸ºåœ¨vscodeä¸­ç›´æ¥è¿è¡Œæ²¡æœ‰è¿”å›å€¼


### å®ä¾‹1 ï¼šrender.htmlç«¯ç‚¹
ä½¿ç”¨requestsåº“è°ƒç”¨render.htmlç«¯ç‚¹æœåŠ¡å¯¹é¡µé¢ [http://quotes.toscrape.com/js/](http://quotes.toscrape.com/js/) è¿›è¡Œæ¸²æŸ“çš„ç¤ºä¾‹ä»£ç  ã€‚
åœ¨ç»ˆç«¯ä¸­è¿›å…¥pythonçš„ç¼–è¯‘ç¯å¢ƒï¼Œè¾“å…¥ä»¥ä¸‹ä»£ç 
```python
import requests
from scrapy.selector import Selector

splash_url = 'http://localhost:8050/render.html'
args = {'url': 'http://quotes.toscrape.com/js', 'timeout': 60, 'image': 0}

response = requests.get(splash_url, params=args)

sel = Selector(response)
sel.css('div.quote span.text::text').extract()
```
è¿è¡ŒæˆåŠŸåå³å–åˆ°äº†é¡µé¢ä¸Šçš„åäººåè¨€ï¼Œè¿è¡Œç»“æœå¦‚å›¾
![image.png](https://cdn.nlark.com/yuque/0/2021/png/638254/1624605796185-0974c6eb-abbe-435f-8b4e-99e8e8bde0be.png#clientId=u90185fc2-b9f5-4&from=paste&height=754&id=u6e455689&margin=%5Bobject%20Object%5D&name=image.png&originHeight=754&originWidth=1266&originalType=binary&ratio=1&size=483311&status=done&style=none&taskId=uae26be9d-ea3e-4503-b1bb-55dfc8c6234&width=1266)
### å®ä¾‹2ï¼šexecuteç«¯ç‚¹
åœ¨çˆ¬å–æŸäº›é¡µé¢æ—¶ï¼Œæˆ‘ä»¬æƒ³åœ¨é¡µé¢ä¸­æ‰§è¡Œä¸€äº›ç”¨æˆ·è‡ªå®šä¹‰çš„JavaScriptä»£ç ï¼Œä¾‹å¦‚ï¼Œç”¨JavaScriptæ¨¡æ‹Ÿç‚¹å‡»é¡µé¢ä¸­çš„æŒ‰é’®ï¼Œæˆ–è°ƒç”¨é¡µé¢ä¸­çš„JavaScriptå‡½æ•°ä¸æœåŠ¡å™¨äº¤äº’ï¼Œåˆ©ç”¨Splashçš„executeç«¯ç‚¹æä¾›çš„æœåŠ¡å¯ä»¥å®ç°è¿™æ ·çš„åŠŸèƒ½ã€‚
â€‹

splash_executeè„šæœ¬ï¼šåˆ©ç”¨Splashçš„executeç«¯ç‚¹æä¾›çš„æœåŠ¡å¯ä»¥å®ç°åœ¨é¡µé¢ä¸­æ‰§è¡Œä¸€äº›ç”¨æˆ·è‡ªå®šä¹‰JavaScriptä»£ç è‡ªå®šä¹‰çš„åŠŸèƒ½
â€‹

ä¸‹é¢æ˜¯ä½¿ç”¨requestsåº“è°ƒç”¨executeç«¯ç‚¹æœåŠ¡çš„ç¤ºä¾‹ä»£ç ã€‚
```python
import requests
import json

# è¿™æ®µè„šæœ¬éœ€è¦åœ¨å‘½ä»¤è¡Œè¿è¡Œ
lua_script = '''
function main(splash)
    splash:go("http://quotes.toscrape.com") --æ‰“å¼€é¡µé¢
    splash:wait(0.5) --ç­‰å¾…æ—¶é—´
    local title = splash:evaljs("document.title") --æ‰§è¡Œjsä»£ç è·å–ç»“æœ
    return {title = title} --è¿”å›jsonå½¢å¼çš„ç»“æœ
    end
'''

splash_url = 'http://localhost:8050/execute'
headers={'content-type':'application/json', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36'}
data = json.dumps({'lua_source':lua_script})
# args = {'url': 'http://quotes.toscrape.com/js', 'timeout': 60, 'image': 0}

response = requests.post(splash_url, headers=headers, data=data)
response.content

response.json()
```
è¿è¡Œç»“æœå¦‚å›¾ï¼š
![image.png](https://cdn.nlark.com/yuque/0/2021/png/638254/1624606136457-bc0e765f-8ef8-4cda-9167-7fe1765088a3.png#clientId=u90185fc2-b9f5-4&from=paste&height=914&id=u80084fda&margin=%5Bobject%20Object%5D&name=image.png&originHeight=914&originWidth=1254&originalType=binary&ratio=1&size=464666&status=done&style=none&taskId=u647d5a1c-455c-4bb4-888c-7519b0cf035&width=1254)
## åœ¨Scrapyä¸­ä½¿ç”¨Splash


åœ¨Scrapyä¸­è°ƒç”¨SplashæœåŠ¡éœ€è¦ç”¨åˆ°Pythonåº“çš„scrapy-splash
`pip install scrapy-splash`


åˆ›å»ºä¸€ä¸ªScrapyé¡¹ç›®ï¼Œå–åä¸ºsplash_examplesï¼š
`scrapy startproject quotes`


é¦–å…ˆåœ¨é¡¹ç›®é…ç½®æ–‡ä»¶settings.pyä¸­å¯¹scrapy-splashè¿›è¡Œé…ç½®ï¼Œæ·»åŠ å†…å®¹å¦‚ä¸‹ï¼š


```python
# SplashæœåŠ¡å™¨åœ°å€
SPLASH_URL = 'http://localhost:8050'

# å¼€å¯Splashçš„ä¸¤ä¸ªä¸‹è½½ä¸­é—´ä»¶å¹¶è°ƒæ•´HttpCompressionMiddlewareçš„æ¬¡åº
DOWNLOADER_MIDDLEWARES = {
   'scrapy_splash.SplashCookiesMiddleware': 723,
   'scrapy_splash.SplashMiddleware': 725,
   'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,
}

# è®¾ç½®å»é‡è¿‡æ»¤å™¨
DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'

# ç”¨æ¥æ”¯æŒcache_argsï¼ˆå¯é€‰ï¼‰
SPIDER_MIDDLEWARES = {
    'scrapy_splash.SplashDeduplicateArgsMiddleware': 100
}
```


ç¼–å†™Spiderä»£ç è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨scrapy_splashè°ƒç”¨SplashæœåŠ¡éå¸¸ç®€å•ï¼Œscrapy_splashä¸­å®šä¹‰äº†ä¸€ä¸ªSplashRequestç±»ï¼Œç”¨æˆ·åªéœ€ä½¿ç”¨scrapy_splash.SplashRequestï¼ˆæ›¿ä»£scrapy.Requestï¼‰æäº¤è¯·æ±‚å³å¯ã€‚


### å®æˆ˜ä¸€ï¼šçˆ¬å–toscrapeä¸­çš„åäººåè¨€


åˆ›å»ºä¸€ä¸ªquoteçš„çˆ¬è™«
`scrapy genspider quote quotes.toscrape.com`
åœ¨è¿™ä¸ªæ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬åªéœ€ä½¿ç”¨Splashçš„render.htmlç«¯ç‚¹æ¸²æŸ“é¡µé¢ï¼Œå†è¿›è¡Œçˆ¬å–å³å¯å®ç°QuoteSpiderï¼Œå…·ä½“ä»£ç è§spiders/quote.pyã€‚
```python
import scrapy
from scrapy_splash import SplashRequest

class QuoteSpider(scrapy.Spider):
    name = 'quote'
    allowed_domains = ['quotes.toscrape.com']
    start_urls = ['http://quotes.toscrape.com/js/']

    def start_requests(self):
        for url in self.start_urls:
            yield SplashRequest(url, args={'images':0, 'timeout': 30})

    def parse(self, response):
        for sel in response.css('div.quote'):
            quote = sel.css('span.text::text').extract_first()
            author= sel.css('small.author::text').extract_first(),
            # tags= sel.css('.tag::text').extract()
            yield {'quote':quote,'author':author}
            href = response.css('li.next > a::attr(href)').extract_first()
            if href:
                url = response.urljoin(href)
                yield SplashRequest(url, args = {'images': 0, 'timeout': 30})
```


è¿è¡Œè¿™ä¸ªçˆ¬è™«å¹¶ä¿å­˜æ–‡ä»¶
`scrapy crawl quote -o quote.csv`
â€‹

å¯ä»¥çœ‹åˆ°æˆåŠŸè·å–äº†ç½‘ç«™ä¸‹çš„åäººåè¨€ã€‚


### å®æˆ˜äºŒï¼šçˆ¬å–äº¬ä¸œå•†åŸä¸­çš„ä¹¦ç±ä¿¡æ¯


åœ¨äº¬ä¸œå•†åŸä¸­çš„ä¹¦ç±åˆ—è¡¨ä½¿ç”¨äº†åŠ¨æ€æ¸²æŸ“æ–¹å¼ï¼Œå³é¡µé¢ä¸­çš„60æœ¬ä¹¦ä¸æ˜¯åŒæ—¶åŠ è½½çš„ï¼Œå¼€å§‹åªæœ‰30æœ¬ä¹¦ï¼Œå½“æˆ‘ä»¬ä½¿ç”¨é¼ æ ‡æ»šè½®æ»šåŠ¨åˆ°é¡µé¢ä¸‹æ–¹æŸä½ç½®æ—¶ï¼Œå30æœ¬ä¹¦æ‰ç”±JavaScriptè„šæœ¬åŠ è½½ã€‚


æ—¢ç„¶å¦‚æ­¤ï¼Œçˆ¬å–è¿™ä¸ªé¡µé¢æ—¶ï¼Œå¯ä»¥å…ˆæ‰§è¡Œä¸€æ®µJavaScriptä»£ç ï¼Œå°†æ»šåŠ¨æ¡æ‹–åˆ°é¡µé¢ä¸‹æ–¹æŸä½ç½®ï¼Œè§¦å‘åŠ è½½å30æœ¬ä¹¦çš„äº‹ä»¶


åˆ›å»ºä¸€ä¸ªjdbookçš„çˆ¬è™«ï¼š
`scrapy genspider jdbook search.jd.com`


ç»ä¸Šè¿°åˆ†æï¼Œåœ¨çˆ¬å–æ¯ä¸€ä¸ªä¹¦ç±åˆ—è¡¨é¡µé¢æ—¶éƒ½éœ€è¦æ‰§è¡Œä¸€æ®µJavaScriptä»£ç ï¼Œä»¥è®©å…¨éƒ¨ä¹¦ç±åŠ è½½ï¼Œå› æ­¤é€‰ç”¨executeç«¯ç‚¹å®Œæˆè¯¥ä»»åŠ¡ï¼Œå®ç°JDBookSpiderä»£ç è§spiders/jdbook.pyã€‚
â€‹

```python
import scrapy
from scrapy import Request
from scrapy_splash import SplashRequest

lua_script = '''
function main(splash)
    splash:go(splash.args.url)
    splash:wait(2)
    splash:runjs("document.getElementsByClassName('page')[0].scrollIntoView(true)")
    splash:wait(2)
    return splash:html()
end
'''

class JdbookSpider(scrapy.Spider):
    name = 'jdbook'
    # allowed_domains = ['search.jd.book']
    base_url = 'https://search.jd.com/Search?keyword=python&enc=utf-8&wq=python'

    def start_requests(self):
        yield Request(self.base_url, callback=self.parse_urls, dont_filter=True)

    def parse_urls(self, response):
        # è·å–å•†å“æ€»æ•°ï¼Œè®¡ç®—å‡ºæ€»é¡µæ•°
        # total = int(response.css('span#J_resCount::text').extract_first())
        # pageNum = total // 60 + (1 if total % 60 else 0)
        # pageNum = response.css('.fp-text i::text').extract_first() || 100
        pageNum = 10

        # æ„é€ æ¯é¡µçš„urlï¼Œå‘Splashçš„executeç«¯ç‚¹å‘é€è¯·æ±‚
        for i in range(pageNum):
            url = '%s&page=%s' % (self.base_url, 2*i+1)
            yield SplashRequest(url, 
                endpoint='execute', 
                args={'lua_source': lua_script},
                cache_args=['lua_source'])

    def parse(self, response):
        # è·å–ä¸€ä¸ªé¡µé¢ä¸­æ¯æœ¬ä¹¦çš„åå­—å’Œä»·æ ¼
        for sel in response.css('ul.gl-warp.clearfix > li.gl-item'):
            yield {
                'name': sel.css('div.p-name').xpath('string(.//em)').extract_first(),
                'price': sel.css('div.p-price i::text').extract_first(),
            }

```


è¿è¡Œè¿™ä¸ªçˆ¬è™«å¹¶ä¿å­˜æ–‡ä»¶
`scrapy crawl jdbook -o books.csv`
â€‹

å¯ä»¥çœ‹åˆ°æˆåŠŸè·å–äº†ç½‘ç«™ä¸‹çš„æ‰€æœ‰å›¾ä¹¦çš„ä¿¡æ¯ã€‚


æºç åœ°å€ï¼š[splash-demo](https://github.com/skique/splash-demo)   
åˆ«å¿˜äº†ç‚¹é¢—å°æ˜Ÿæ˜ŸğŸŒŸğŸŒŸ

## å‚è€ƒèµ„æ–™

- ã€Šç²¾é€šScrapyç½‘ç»œçˆ¬è™«ã€‹ç¬¬11ç« 
- ã€ŠPython 3ç½‘ç»œçˆ¬è™«å®æˆ˜ã€‹ç¬¬7ç« 
